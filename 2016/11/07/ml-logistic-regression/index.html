<!DOCTYPE html>
<html lang="en">

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Machine Learning: Logistic Regression - Side Notes</title>

  <link rel="icon" href="/favicon.ico">
  <link rel="stylesheet" href="/assets/main.css">
  <link rel="canonical" href="">
  <link rel="alternate" type="application/rss+xml" title="Side Notes" href="">

  <link href='/stylesheets/all-3946f92fb0c0a9c01c341f255decb03c.css' media='all' rel='stylesheet' type='text/css'>

  <!-- MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>

<script type="text/javascript"
    src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML'></script>

</head>

  <body>

    <header class="site-header" role="banner">

  <div class="wrapper">
    
    
    <a class="site-title" href="/">Side Notes</a>
  
    
      <nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path fill="#424242" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
              <path fill="#424242" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
              <path fill="#424242" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger">
          
            
          
            
            <a class="page-link" href="/about/">About</a>
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
        </div>
      </nav>
    
  </div>
</header>

    <main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title" itemprop="name headline">Machine Learning: Logistic Regression</h1>
    <p class="post-meta">
      <time datetime="2016-11-07T16:05:31-05:00" itemprop="datePublished">
        
        Nov 7, 2016
      </time>
      
      
        • <span>Categories: Machine Learning</span>
      
      
        • <span>Tags: machine learning, math, matlab</span>
      </p>
  </header>

  <div class="post-content" itemprop="articleBody">
    <p><em>Logistic regression</em> is a classification case of <a href="/2016/10/28/ml-linear-regression">linear regression</a> whith
dependent variable $y$ taking binary values.</p>

<p>Problem: Given a training set $\langle x^{(i)}, y^{(i)} \rangle$, $1 \le i \le m$,
$x \in \mathbb{R}^{n+1}$, $x^{(i)} _ 0 = 0$, $y^{(i)} \in $ {0,1},
find <em>classification function</em></p>

<script type="math/tex; mode=display">h_\theta(x) = P(y = 1 | x; \theta)</script>

<!-- more -->

<h2 id="gradient-descent">Gradient Descent</h2>

<p>Let’s build function $h_\theta(x)$ as a sigmoid function of $\theta\cdot x$</p>

<script type="math/tex; mode=display">h_\theta(x) = g(\theta\cdot x), \quad
g(z) = \mathbb{sigmoid}(z) = \frac{1}{1 + e^{-z}}</script>

<p>Sigmoid function has rank infinity, i.e. it operates on scalars, vectors and matrices.</p>

<figure class="code-highlight-figure"><figcaption class="code-highlight-caption"><span class="code-highlight-caption-title">sigmoid.m</span></figcaption><div class="code-highlight"><pre class="code-highlight-pre"><div data-line="1" class="code-highlight-row numbered"><div class="code-highlight-line"><span class="k">function</span> <span class="n">g</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
</div></div><div data-line="2" class="code-highlight-row numbered"><div class="code-highlight-line">    <span class="n">g</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">.</span><span class="p">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="nb">exp</span><span class="p">(</span><span class="o">-</span><span class="n">z</span><span class="p">));</span>
</div></div><div data-line="3" class="code-highlight-row numbered"><div class="code-highlight-line"><span class="k">end</span></div></div></pre></div></figure>

<p>To find optimal parameter $\theta \in \mathbb{R}^{n+1}$ we are going to use
optimized gradient descent method which takes as arguments
cost function $J(\theta)$ and its gradient.
For logistic regression they are</p>

<script type="math/tex; mode=display">J(\theta) = -\frac{1}{m} \left( y^T \ln h_\theta(X) + (1-y)^T \ln (1-h_\theta(X)) \right) \\
\nabla J(\theta) = \frac{1}{m} X^T (h_\theta(X) - y)</script>

<p>where $X = (x^{(i)}_j) _{m \times n+1}$ is a matrix of the training examples from
the <a href="/2016/10/28/ml-linear-regression">previous lecture</a>.</p>

<p>Analogous to linear regression, logistic regression can be regularized too</p>

<script type="math/tex; mode=display">J(\theta) = -\frac{1}{m} \left( y^T \ln h_\theta(X) + (1-y)^T \ln (1-h_\theta(X)) \right) + \frac{\lambda}{2m} \| E\theta \|^2 \\
\nabla J(\theta) = \frac{1}{m} \left( X^T (h_\theta(X) - y) + \lambda E\theta \right)</script>

<figure class="code-highlight-figure"><figcaption class="code-highlight-caption"><span class="code-highlight-caption-title">costFunction.m</span></figcaption><div class="code-highlight"><pre class="code-highlight-pre"><div data-line="1" class="code-highlight-row numbered"><div class="code-highlight-line"><span class="k">function</span> <span class="p">[</span><span class="n">J</span><span class="p">,</span> <span class="n">grad</span><span class="p">]</span> <span class="o">=</span> <span class="n">costFunction</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">lambda</span><span class="p">)</span>
</div></div><div data-line="2" class="code-highlight-row numbered"><div class="code-highlight-line">    <span class="n">m</span> <span class="o">=</span> <span class="nb">length</span><span class="p">(</span><span class="n">y</span><span class="p">);</span> <span class="c1">% number of training examples</span>
</div></div><div data-line="3" class="code-highlight-row numbered"><div class="code-highlight-line">    <span class="n">h</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">X</span> <span class="o">*</span> <span class="n">theta</span><span class="p">);</span>
</div></div><div data-line="4" class="code-highlight-row numbered"><div class="code-highlight-line">    <span class="n">J</span> <span class="o">=</span> <span class="p">(</span><span class="n">y</span><span class="s1">' * log(h) + (1 - y)'</span> <span class="o">*</span> <span class="nb">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">h</span><span class="p">))</span> <span class="p">/</span> <span class="o">-</span><span class="n">m</span><span class="p">;</span>
</div></div><div data-line="5" class="code-highlight-row numbered"><div class="code-highlight-line">    <span class="n">grad</span> <span class="o">=</span> <span class="n">X</span><span class="o">'</span> <span class="o">*</span> <span class="p">(</span><span class="n">h</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span> <span class="p">/</span> <span class="n">m</span><span class="p">;</span>
</div></div><div data-line="6" class="code-highlight-row numbered"><div class="code-highlight-line"> </div></div><div data-line="7" class="code-highlight-row numbered"><div class="code-highlight-line">    <span class="c1">% Regularization</span>
</div></div><div data-line="8" class="code-highlight-row numbered"><div class="code-highlight-line">    <span class="n">th</span> <span class="o">=</span> <span class="n">theta</span><span class="p">;</span> <span class="n">th</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
</div></div><div data-line="9" class="code-highlight-row numbered"><div class="code-highlight-line"> </div></div><div data-line="10" class="code-highlight-row numbered"><div class="code-highlight-line">    <span class="n">J</span> <span class="o">=</span> <span class="n">J</span> <span class="o">+</span> <span class="n">th</span><span class="o">'</span> <span class="o">*</span> <span class="n">th</span> <span class="o">*</span> <span class="n">lambda</span> <span class="p">/</span> <span class="n">m</span> <span class="p">/</span> <span class="mi">2</span><span class="p">;</span>
</div></div><div data-line="11" class="code-highlight-row numbered"><div class="code-highlight-line">    <span class="n">grad</span> <span class="o">=</span> <span class="n">grad</span> <span class="o">+</span> <span class="n">th</span> <span class="o">*</span> <span class="n">lambda</span> <span class="p">/</span> <span class="n">m</span><span class="p">;</span>
</div></div><div data-line="12" class="code-highlight-row numbered"><div class="code-highlight-line"><span class="k">end</span></div></div></pre></div></figure>

<p>Having computed $\theta$ we can now implement the prediction function</p>

<figure class="code-highlight-figure"><figcaption class="code-highlight-caption"><span class="code-highlight-caption-title">predict.m</span></figcaption><div class="code-highlight"><pre class="code-highlight-pre"><div data-line="1" class="code-highlight-row numbered"><div class="code-highlight-line"><span class="k">function</span> <span class="n">p</span> <span class="o">=</span> <span class="n">predict</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span>
</div></div><div data-line="2" class="code-highlight-row numbered"><div class="code-highlight-line">    <span class="n">p</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">X</span> <span class="o">*</span> <span class="n">theta</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="mf">0.5</span><span class="p">;</span>
</div></div><div data-line="3" class="code-highlight-row numbered"><div class="code-highlight-line"><span class="k">end</span></div></div></pre></div></figure>

<p>which can be used to classify new examples and check the prediction accuracy
on the training set</p>

<figure class="code-highlight-figure"><figcaption class="code-highlight-caption"><span class="code-highlight-caption-title">accuracy.m</span></figcaption><div class="code-highlight"><pre class="code-highlight-pre"><div data-line="1" class="code-highlight-row numbered"><div class="code-highlight-line"><span class="k">function</span> <span class="n">a</span> <span class="o">=</span> <span class="n">accuracy</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</div></div><div data-line="2" class="code-highlight-row numbered"><div class="code-highlight-line">    <span class="n">p</span> <span class="o">=</span> <span class="n">predict</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">X</span><span class="p">);</span>
</div></div><div data-line="3" class="code-highlight-row numbered"><div class="code-highlight-line">    <span class="n">a</span> <span class="o">=</span> <span class="nb">mean</span><span class="p">(</span><span class="nb">double</span><span class="p">(</span><span class="n">p</span> <span class="o">==</span> <span class="n">y</span><span class="p">))</span> <span class="o">*</span> <span class="mi">100</span><span class="p">;</span>
</div></div><div data-line="4" class="code-highlight-row numbered"><div class="code-highlight-line"><span class="k">end</span></div></div></pre></div></figure>

<h2 id="multi-class-classification">Multi-class Classification</h2>

<p>Logistic regression works for binary $y$.
Suppose now that $y^{(i)} \in ${$1,…,K$}, where $K &gt; 2$.
In this case we can use <em>One-vs-All</em> variation of this algorithm.</p>

<p>Step 1. Convert vector $y$ into a binary matrix $Y$</p>

<script type="math/tex; mode=display">% <![CDATA[
y =
\begin{pmatrix}
y^{(1)} \\
y^{(2)} \\
y^{(3)} \\
\vdots \\
y^{(m)} \\
\end{pmatrix}
\quad
\rightarrow
\quad
Y =
\begin{pmatrix}
y^{(1)}_1 & y^{(1)}_2 & \cdots & y^{(1)}_K \\
y^{(2)}_1 & y^{(2)}_2 & \cdots & y^{(2)}_K \\
y^{(3)}_1 & y^{(3)}_2 & \cdots & y^{(3)}_K \\
\vdots & \vdots & \ddots & \vdots \\
y^{(m)}_1 & y^{(m)}_2 & \cdots & y^{(m)}_K \\
\end{pmatrix} %]]></script>

<p>where $y^{(i)}_k = \delta _{k y^{(i)}}$, i.e. $y^{(i)}_k = 1$ when $y^{(i)} = k$, otherwise $y^{(i)}_k = 0$.</p>

<p>Step 2. Train logistic classifier on every column of matrix $Y$.
The result will be a matrix $\Theta = (\theta_{jk})_{n+1 \times K}$</p>

<script type="math/tex; mode=display">% <![CDATA[
\Theta =
\begin{pmatrix}
\theta_{01} & \theta_{02} & \cdots & \theta_{0K} \\
\theta_{11} & \theta_{12} & \cdots & \theta_{1K} \\
\theta_{21} & \theta_{22} & \cdots & \theta_{2K} \\
\vdots & \vdots & \ddots & \vdots \\
\theta_{n1} & \theta_{n2} & \cdots & \theta_{nK} \\
\end{pmatrix} %]]></script>

<p>Step 3. For any given vector $x$ compute vector $h = x^T \Theta$. Then
the predicted value $y$ will be</p>

<script type="math/tex; mode=display">y = \{ p \: | \: h_p = \mathbb{max} (h_k), 1 \le k \le K \}</script>

<p>To compute accuracy of the one-vs-all classifier on the training set
use <code class="highlighter-rouge">accuracy.m</code> script from above with modified <code class="highlighter-rouge">predict.m</code></p>

<figure class="code-highlight-figure"><figcaption class="code-highlight-caption"><span class="code-highlight-caption-title">predict.m</span></figcaption><div class="code-highlight"><pre class="code-highlight-pre"><div data-line="1" class="code-highlight-row numbered"><div class="code-highlight-line"><span class="k">function</span> <span class="n">p</span> <span class="o">=</span> <span class="n">predict</span><span class="p">(</span><span class="n">Theta</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span>
</div></div><div data-line="2" class="code-highlight-row numbered"><div class="code-highlight-line">    <span class="n">a</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">X</span> <span class="o">*</span> <span class="n">Theta</span><span class="p">);</span>
</div></div><div data-line="3" class="code-highlight-row numbered"><div class="code-highlight-line">    <span class="p">[</span><span class="n">v</span><span class="p">,</span> <span class="n">p</span><span class="p">]</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="p">[],</span> <span class="mi">2</span><span class="p">);</span>
</div></div><div data-line="4" class="code-highlight-row numbered"><div class="code-highlight-line"><span class="k">end</span></div></div></pre></div></figure>


  </div>

  
</article>

      </div>
    </main>

    <footer class="site-footer">

  <div class="wrapper">

    <div class="footer-credit">

      <span class="footer-credit-left">
        Copyright &copy; 2009–2019 Andrey Paramonov
      </span>

      <span class="footer-credit-right">
        Powered by <a href="https://github.com/octopress/octopress">Octopress</a> •
        Hosted by <a href="https://github.com">GitHub</a>
      </span>

    </div>

  </div>

</footer>

  </body>

</html>
