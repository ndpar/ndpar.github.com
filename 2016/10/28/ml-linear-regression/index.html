<!DOCTYPE html>
<html lang="en">

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Machine Learning: Linear Regression - Side Notes</title>

  <link rel="icon" href="/favicon.ico">
  <link rel="stylesheet" href="/assets/main.css">
  <link rel="canonical" href="">
  <link rel="alternate" type="application/rss+xml" title="Side Notes" href="">

  <link href='/stylesheets/all-3946f92fb0c0a9c01c341f255decb03c.css' media='all' rel='stylesheet' type='text/css'>

  <!-- MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>

<script type="text/javascript"
    src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML'></script>

</head>

  <body>

    <header class="site-header" role="banner">

  <div class="wrapper">
    
    
    <a class="site-title" href="/">Side Notes</a>
  
    
      <nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path fill="#424242" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
              <path fill="#424242" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
              <path fill="#424242" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger">
          
            
          
            
            <a class="page-link" href="/about/">About</a>
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
        </div>
      </nav>
    
  </div>
</header>

    <main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title" itemprop="name headline">Machine Learning: Linear Regression</h1>
    <p class="post-meta">
      <time datetime="2016-10-28T17:05:31-04:00" itemprop="datePublished">
        
        Oct 28, 2016
      </time>
      
      
        • <span>Categories: Machine Learning</span>
      
      
        • <span>Tags: machine learning, math, matlab</span>
      </p>
  </header>

  <div class="post-content" itemprop="articleBody">
    <p>Let $y$ be a dependent variable of a feature vector $x$</p>

<script type="math/tex; mode=display">x =
\begin{pmatrix}
1 \\
x_1 \\
x_2 \\
\vdots \\
x_n \\
\end{pmatrix}</script>

<p>Problem: Given a training set $\langle x^{(i)}, y^{(i)} \rangle$, $1 \le i \le m$,
find the value of $y$ on any input vector $x$.</p>

<p>We solve this problem by constructing a <em>hypothesis funciton</em> $h_\theta(x)$
using one of the methods below.</p>

<!-- more -->

<h2 id="notation">Notation</h2>

<script type="math/tex; mode=display">% <![CDATA[
x =
\begin{pmatrix}
x_0 \\
x_1 \\
x_2 \\
\vdots \\
x_n \\
\end{pmatrix}
=
\begin{pmatrix}
1 \\
x_1 \\
x_2 \\
\vdots \\
x_n \\
\end{pmatrix}
\quad
X =
\begin{pmatrix}
1 & x^{(1)}_1 & x^{(1)}_2 & \cdots & x^{(1)}_n \\
1 & x^{(2)}_1 & x^{(2)}_2 & \cdots & x^{(2)}_n \\
1 & x^{(3)}_1 & x^{(3)}_2 & \cdots & x^{(3)}_n \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
1 & x^{(m)}_1 & x^{(m)}_2 & \cdots & x^{(m)}_n \\
\end{pmatrix}
\quad
y =
\begin{pmatrix}
y^{(1)} \\
y^{(2)} \\
y^{(3)} \\
\vdots \\
y^{(m)} \\
\end{pmatrix} %]]></script>

<h2 id="optimization-objective">Optimization Objective</h2>

<p>Step 1. Normalize each feature $(x^{(0)}_j, …, x^{(m)}_j)$, $1 \le j \le n$ by mean $\mu$ and standard deviation $\sigma$</p>

<script type="math/tex; mode=display">x^{(i)}_j \leftarrow \frac{x^{(i)}_j - \mu_j}{\sigma_j}</script>

<figure class="code-highlight-figure"><figcaption class="code-highlight-caption"><span class="code-highlight-caption-title">featureNormalize.m</span></figcaption><div class="code-highlight"><pre class="code-highlight-pre"><div data-line="1" class="code-highlight-row numbered"><div class="code-highlight-line"><span class="k">function</span> <span class="p">[</span><span class="n">X_norm</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="p">]</span> <span class="o">=</span> <span class="n">featureNormalize</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</div></div><div data-line="2" class="code-highlight-row numbered"><div class="code-highlight-line">    <span class="n">mu</span> <span class="o">=</span> <span class="nb">mean</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="mi">1</span><span class="p">);</span>
</div></div><div data-line="3" class="code-highlight-row numbered"><div class="code-highlight-line">    <span class="n">sigma</span> <span class="o">=</span> <span class="nb">std</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">);</span>
</div></div><div data-line="4" class="code-highlight-row numbered"><div class="code-highlight-line">    <span class="n">X_norm</span> <span class="o">=</span> <span class="p">(</span><span class="n">X</span> <span class="o">-</span> <span class="n">mu</span><span class="p">)</span> <span class="o">.</span><span class="p">/</span> <span class="n">sigma</span><span class="p">;</span>
</div></div><div data-line="5" class="code-highlight-row numbered"><div class="code-highlight-line"><span class="k">end</span></div></div></pre></div></figure>

<p>Step 2. Minimize the cost function</p>

<script type="math/tex; mode=display">J(\theta) = \frac{1}{2m} \|X\theta - y\|^2 = \frac{1}{2m}(X\theta - y)^T (X\theta - y)</script>

<p>where $\theta = (\theta_0, …, \theta_n)^T$</p>

<p>Step 3. Compute hypothesis function as</p>

<script type="math/tex; mode=display">h_\theta(x) = x \cdot \theta = x^T \theta</script>

<p>where vector $x$ is normalized using the same values of $\mu$ and $\sigma$ as in Step 1.</p>

<h2 id="gradient-descent">Gradient Descent</h2>

<p><em>Gradient descent</em> is the method for finding (global) minimum of cost funtion $J(\theta)$.
There are few ways to implement this method.</p>

<h3 id="direct-method">Direct method</h3>

<p>Choose small learning rate $\alpha &gt; 0$ and find the fixed point of the function</p>

<script type="math/tex; mode=display">f(\theta) = \theta - \frac{\alpha}{m}X^T (X\theta - y)</script>

<figure class="code-highlight-figure"><figcaption class="code-highlight-caption"><span class="code-highlight-caption-title">gradientDescent.m</span></figcaption><div class="code-highlight"><pre class="code-highlight-pre"><div data-line="1" class="code-highlight-row numbered"><div class="code-highlight-line"><span class="k">function</span> <span class="p">[</span><span class="n">theta</span><span class="p">]</span> <span class="o">=</span> <span class="n">gradientDescent</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="nb">alpha</span><span class="p">,</span> <span class="n">num_iters</span><span class="p">)</span>
</div></div><div data-line="2" class="code-highlight-row numbered"><div class="code-highlight-line">    <span class="n">m</span> <span class="o">=</span> <span class="nb">length</span><span class="p">(</span><span class="n">y</span><span class="p">);</span>
</div></div><div data-line="3" class="code-highlight-row numbered"><div class="code-highlight-line">    <span class="k">for</span> <span class="n">iter</span> <span class="o">=</span> <span class="mi">1</span><span class="p">:</span><span class="n">num_iters</span>
</div></div><div data-line="4" class="code-highlight-row numbered"><div class="code-highlight-line">        <span class="n">h</span> <span class="o">=</span> <span class="n">X</span> <span class="o">*</span> <span class="n">theta</span><span class="p">;</span>
</div></div><div data-line="5" class="code-highlight-row numbered"><div class="code-highlight-line">        <span class="n">delta</span> <span class="o">=</span> <span class="n">h</span> <span class="o">-</span> <span class="n">y</span><span class="p">;</span>
</div></div><div data-line="6" class="code-highlight-row numbered"><div class="code-highlight-line">        <span class="n">theta</span> <span class="o">=</span> <span class="n">theta</span> <span class="o">-</span> <span class="n">X</span><span class="o">'</span> <span class="o">*</span> <span class="n">delta</span> <span class="o">*</span> <span class="nb">alpha</span> <span class="p">/</span> <span class="n">m</span><span class="p">;</span>
</div></div><div data-line="7" class="code-highlight-row numbered"><div class="code-highlight-line">    <span class="k">end</span>
</div></div><div data-line="8" class="code-highlight-row numbered"><div class="code-highlight-line"><span class="k">end</span></div></div></pre></div></figure>

<h3 id="optimized-method">Optimized method</h3>

<p>Many mathematical software packages already include implementations of gradient
descent that compute learning rate $\alpha$ automatically.
These methods accept cost function $J(\theta)$ and its gradient
$\nabla J(\theta)$ as arguments, which for the linear regression is computed as follows</p>

<script type="math/tex; mode=display">\nabla J(\theta) = \frac{1}{m}X^T (X\theta - y)</script>

<figure class="code-highlight-figure"><figcaption class="code-highlight-caption"><span class="code-highlight-caption-title">costFunction.m</span></figcaption><div class="code-highlight"><pre class="code-highlight-pre"><div data-line="1" class="code-highlight-row numbered"><div class="code-highlight-line"><span class="k">function</span> <span class="p">[</span><span class="n">J</span><span class="p">,</span> <span class="n">grad</span><span class="p">]</span> <span class="o">=</span> <span class="n">costFunction</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</div></div><div data-line="2" class="code-highlight-row numbered"><div class="code-highlight-line">    <span class="n">m</span> <span class="o">=</span> <span class="nb">length</span><span class="p">(</span><span class="n">y</span><span class="p">);</span>
</div></div><div data-line="3" class="code-highlight-row numbered"><div class="code-highlight-line">    <span class="n">h</span> <span class="o">=</span> <span class="n">X</span> <span class="o">*</span> <span class="n">theta</span><span class="p">;</span>
</div></div><div data-line="4" class="code-highlight-row numbered"><div class="code-highlight-line">    <span class="n">delta</span> <span class="o">=</span> <span class="n">h</span> <span class="o">-</span> <span class="n">y</span><span class="p">;</span>
</div></div><div data-line="5" class="code-highlight-row numbered"><div class="code-highlight-line">    <span class="n">J</span> <span class="o">=</span> <span class="n">delta</span><span class="o">'</span> <span class="o">*</span> <span class="n">delta</span> <span class="p">/</span> <span class="mi">2</span> <span class="p">/</span> <span class="n">m</span><span class="p">;</span>
</div></div><div data-line="6" class="code-highlight-row numbered"><div class="code-highlight-line">    <span class="n">grad</span> <span class="o">=</span> <span class="n">X</span><span class="o">'</span> <span class="o">*</span> <span class="n">delta</span> <span class="p">/</span> <span class="n">m</span><span class="p">;</span>
</div></div><div data-line="7" class="code-highlight-row numbered"><div class="code-highlight-line"><span class="k">end</span></div></div></pre></div></figure>

<figure class="code-highlight-figure"><figcaption class="code-highlight-caption"><span class="code-highlight-caption-title">gradientDescent.m</span></figcaption><div class="code-highlight"><pre class="code-highlight-pre"><div data-line="1" class="code-highlight-row numbered"><div class="code-highlight-line"><span class="k">function</span> <span class="p">[</span><span class="n">theta</span><span class="p">]</span> <span class="o">=</span> <span class="n">gradientDescent</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">initial_theta</span><span class="p">)</span>
</div></div><div data-line="2" class="code-highlight-row numbered"><div class="code-highlight-line">    <span class="n">options</span> <span class="o">=</span> <span class="nb">optimset</span><span class="p">(</span><span class="s1">'GradObj'</span><span class="p">,</span> <span class="s1">'on'</span><span class="p">,</span> <span class="s1">'MaxIter'</span><span class="p">,</span> <span class="mi">400</span><span class="p">);</span>
</div></div><div data-line="3" class="code-highlight-row numbered"><div class="code-highlight-line">    <span class="p">[</span><span class="n">theta</span><span class="p">,</span> <span class="n">cost</span><span class="p">]</span> <span class="o">=</span> <span class="n">fminunc</span><span class="p">(</span><span class="o">@</span><span class="p">(</span><span class="n">t</span><span class="p">)(</span><span class="n">costFunction</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)),</span> <span class="n">initial_theta</span><span class="p">,</span> <span class="n">options</span><span class="p">);</span>
</div></div><div data-line="4" class="code-highlight-row numbered"><div class="code-highlight-line"><span class="k">end</span></div></div></pre></div></figure>

<h2 id="normal-equation">Normal Equation</h2>

<p>Unlike Gradient Descent this method does not require feature normalization (Step 1) and convergence loop. <em>Normal equation</em> gives the closed-form solution to linear regression</p>

<script type="math/tex; mode=display">\theta = (X^T X)^{-1} X^T y</script>

<figure class="code-highlight-figure"><figcaption class="code-highlight-caption"><span class="code-highlight-caption-title">normalEquation.m</span></figcaption><div class="code-highlight"><pre class="code-highlight-pre"><div data-line="1" class="code-highlight-row numbered"><div class="code-highlight-line"><span class="k">function</span> <span class="p">[</span><span class="n">theta</span><span class="p">]</span> <span class="o">=</span> <span class="n">normalEquation</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</div></div><div data-line="2" class="code-highlight-row numbered"><div class="code-highlight-line">    <span class="n">theta</span> <span class="o">=</span> <span class="nb">pinv</span><span class="p">(</span><span class="n">X</span><span class="s1">' * X) * X'</span> <span class="o">*</span> <span class="n">y</span>
</div></div><div data-line="3" class="code-highlight-row numbered"><div class="code-highlight-line"><span class="k">end</span></div></div></pre></div></figure>

<h2 id="regularization">Regularization</h2>

<p>In case of overfitting both methods can be tweaked by introducing polynomial
features and adjusting equations as follows. Let $\lambda &gt; 0$ and E be the
diagonal matrix</p>

<script type="math/tex; mode=display">% <![CDATA[
E =
\begin{pmatrix}
0 & & & & \\
& 1 & & & \\
& & 1 & & \\
& & & \ddots & \\
& & & & 1 \\
\end{pmatrix}_{n+1 \times n+1} %]]></script>

<p>Then the cost function for gradient descent becomes</p>

<script type="math/tex; mode=display">J(\theta) = \frac{1}{2m} \left( \|h_\theta(X) - y\|^2 + \lambda \| E\theta \|^2 \right) \\
\nabla J(\theta) = \frac{1}{m} \left( X^T (h_\theta(X) - y) + \lambda E\theta \right)</script>

<p>and normal equation</p>

<script type="math/tex; mode=display">\theta = \left( X^T X +\lambda E \right)^{-1} X^T y</script>

  </div>

  
</article>

      </div>
    </main>

    <footer class="site-footer">

  <div class="wrapper">

    <div class="footer-credit">

      <span class="footer-credit-left">
        Copyright &copy; 2009–2018 Andrey Paramonov
      </span>

      <span class="footer-credit-right">
        Powered by <a href="https://github.com/octopress/octopress">Octopress</a> •
        Hosted by <a href="https://github.com">GitHub</a>
      </span>

    </div>

  </div>

</footer>

  </body>

</html>
